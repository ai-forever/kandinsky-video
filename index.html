<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>KandinskyVideo</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/video-camera.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">KandinskyVideo </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://keunhong.com">Vladimir Arkhipkin</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://utkarshsinha.com">Zein Shaheen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jonbarron.info">Viacheslav Vasilev</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="http://sofienbouaziz.com">Elizaveta Dakhova</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.danbgoldman.com">Andrey Kuznetsov</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://homes.cs.washington.edu/~seitz/">Denis Dimitrov</a><sup>1,</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Sber AI,</span>
            <span class="author-block"><sup>2</sup>Moscow Institute of Physics and Technology</span>
            <span class="author-block"><sup>3</sup>Artificial Intelligence Research Institute</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ai-forever/KandinskyVideo"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/meduza.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fireplace.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/explosion.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/road.mp4"
                    type="video/mp4">
          </video>
        </div>  
      </div>
    </div>
    <h2 class="subtitle has-text-centered">
      <span class="dnerf">FusionFrames is a text-to-video generation model consisting of two main stages - keyframe generation and interpolation.
         <br> Our approach for temporal conditioning allows us to generate videos with high-quality appearance, smoothness and dynamics.
    </h2>
  </div>
</section>

<!-- FusionFrames is a text-to-video generation model consisting of two main stages - keyframe generation and interpolation.  -->
<!-- Our approach for temporal conditioning allows us to generate videos with high-quality appearance, smoothness and dynamics. -->
<!--  -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Multimedia generation approaches occupy a prominent
place in artificial intelligence research. Text-to-image mod-
els achieved high-quality results over the last few years.
However, video synthesis methods recently started to de-
velop. This paper presents a new two-stage latent diffusion
text-to-video generation architecture based on the text-to-
image diffusion model. The first stage concerns keyframes
synthesis to figure the storyline of a video, while the second
one is devoted to interpolation frames generation to make
movements of the scene and objects smooth. We compare
several temporal conditioning approaches for keyframes
generation. The results show the advantage of using sep-
arate temporal blocks over temporal layers in terms of met-
rics reflecting video generation quality aspects and human
preference. The design of our interpolation model sig-
nificantly reduces computational costs compared to other
masked frame interpolation approaches. Furthermore, we
evaluate different configurations of MoVQ-based video de-
coding scheme to improve consistency and achieve higher
PSNR, SSIM, MSE, and LPIPS scores. Finally, we com-
pare our pipeline with existing solutions and achieve top-
2 scores overall and top-1 among open-source solutions:
CLIPSIM = 0.2976 and FVD = 433.054. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overall Pipeline</h2>
          <div class="content has-text-justified">
            <img src="./static/images/pipeline.jpg"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
          <p>
            The encoded text prompt enters the U-Net keyframe generation model with temporal layers or blocks, and then the sampled latent keyframes are  sent to the latent interpolation model in such a way as to predict three interpolation frames between two keyframes. A temporal MoVQ-GAN decoder is used to get the final video result.
          </p>
          </div>
        </div>    
      </div>
    <!--/ Matting. -->

      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Keyframes Generation with Temporal Conditioning </h2>
          <div class="content has-text-justified">
            <img src="./static/images/temporal_layers_blocks.jpg"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
          <p>
            We examined two approaches of temporal components use in pretrained architecture of T2I U-Net â€“ the traditional approach of mixing spatial and temporal layers in one block (left) and our approach of allocating a separate temporal block (middle). 
            All layers indicated in gray are not trained in T2V architectures and are initialized with the weights of the T2I model. 
            NA and N in the left corner of all layers correspond to the presence of prenormalization layers with and without activation, respectively. 
            For different types of blocks we implemented different types of temporal attention and temporal convolution layers, (left). 
            We also implement different types of temporal conditioning. 
            One of them is simple conditioning when pixel see only itself value in different moments of type (1D layers). 
            In 3D layers pixels can see the values of its neighbors also in different moments of time (right).
          </p>
          </div>
        </div>    
      </div>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video Frame Interpolation</h2>
          <div class="content has-text-justified">
            <img src="./static/images/interpolation_ours.jpg"
            class="interpolation-image"
            alt="Interpolate start reference image."/>
          <p>
            A summary of the primary changes made to the T2I architecture includes: 
            (i) The input convolution now accepts three noisy latent inputs (interpolated frames) and two conditioning latent inputs (keyframes). 
            (ii) The output convolution predicts three denoised latents. 
            (iii) Temporal convolutions have been introduced after each spatial convolution.
          </p>
          </div>
        </div>    
      </div>
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results for temporal conditioning</h2>
        <div class="content has-text-justified">
          <img src="./static/images/interpolation_ours.jpg"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        <p>
          A summary of the primary changes made to the T2I architecture includes: 
          (i) The input convolution now accepts three noisy latent inputs (interpolated frames) and two conditioning latent inputs (keyframes). 
          (ii) The output convolution predicts three denoised latents. 
          (iii) Temporal convolutions have been introduced after each spatial convolution.
        </p>
        </div>
      </div>    
    </div>

    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Other results</h2>
        <div class="content has-text-justified">
          <img src="./static/images/interpolation_ours.jpg"
          class="interpolation-image"
          alt="Interpolate start reference image."/>
        <p>
          A summary of the primary changes made to the T2I architecture includes: 
          (i) The input convolution now accepts three noisy latent inputs (interpolated frames) and two conditioning latent inputs (keyframes). 
          (ii) The output convolution predicts three denoised latents. 
          (iii) Temporal convolutions have been introduced after each spatial convolution.
        </p>
        </div>
      </div>    
    </div>

</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
